# 从零开始构建 Rust RPC 框架「一、基本框架」

### 1. 星辰大海

Rust 生态里，RPC 框架的关注度似乎不高，Web 框架倒是有不少。近来笔者愈发觉得还是需要一个稳定可靠高性能的 RPC 框架的。考虑到之前在 [3FS RPC](https://github.com/deepseek-ai/3FS/tree/main/src/common/net) 框架中获得的经验和所犯的错误，我还是决定从零开始构建一套 Rust RPC 框架。这套 RPC 框架有如下目标：

1. 无 proto 文件，通过纯 Rust 定义接口和数据类型；
2. 序列化协议使用 JSON 和 [MessagePack](https://msgpack.org/)，数据包含 schema 能自解释；
3. 传输层协议支持 TCP、WebSocket 和 RDMA，支持前端页面调用 RPC；
4. 高性能、低延迟，优先优化 RDMA 网络场景；
5. 支持 RPC 回调，server 可以调用 client 提供的方法；
6. 完善的可观测性，提供充足的 metrics 和 logs，支持分布式 tracing；
7. 丰富的预定义 service，例如查询 server stats，开箱可用；
8. [可选] 支持网页 API 文档和接口调试工具，支持 JSON Schema；
9. [可选] 支持 Python 接口绑定。

以上这些目标我不确定能否都实现。我会在这个系列的博文里详细介绍我的想法和设计。为了避免烂尾，这个系列的博文我会设置为隐藏，等主要目标都实现了我再一次性公开。

至于项目的名字，我将其命名为 [RuaPC](https://github.com/SF-Zhou/ruapc)。（我猜可能是 Rust Unified Access Procedure Call）

![寓意强大且亲和的喵星人，可 Rua！](../images/9b771e4d3ee9b4d72f951e86b21a19ba.png)

### 2. 基本框架

> 写 RPC 框架，说实话，分分钟写出来，不用一星期，三天！RPC 框架好写，太好写了。 —— Rua 頔

一个最简单的 RPC 框架只需要包含以下部分：

1. 接口定义；
2. 序列化；
3. 连接管理。

思考一下，普通的函数调用（Procedure Call）是怎样的？

```rust
fn echo(name: &str) -> String {
  name.to_string()
}

let value = echo("Rua!");
```

在同一个编译单元里，调用方指定函数名以及约定的参数，就可以调用该函数、获得返回值。RPC（Remote Procedure Call）的步骤也是类似的，只是调用方 caller 和被调用方 callee 可能不在同一个位置，同样是按照约定的接口发起请求，被调用方执行返回结果，caller 获得结果。一次完整的 RPC 的流程如下：

1. [caller] 按照约定的接口定义，指定函数名和参数，发起调用；
2. [caller] 将调用信息和参数序列化为某种格式的载荷（payload），大概率是字节流；
3. [caller] 将载荷通过载体发送给被调用方，载体可以是 TCP 连接；
4. [callee] 接收到载体传递来的载荷；
5. [callee] 将载荷反序列化为调用信息和参数；
6. [callee] 按照请求的调用信息，找到接口定义对应的函数，传入请求的参数并调用；
7. [callee] 获得函数调用的结果，将调用信息和结果序列化为载荷；
8. [callee] 将载荷通过载体发送给调用方；
9. [caller] 接收到载体传递来的载荷；
10. [caller] 将载荷反序列化为约定的结果类型，调用完成。

可以发现，调用方和被调用方的步骤是高度对称的。

### 3. 接口定义

Rust 的 trait 类型非常适合定义接口，例如：

```rust
pub trait EchoService {
    async fn echo(&self, r: &String) -> Result<String>;
    async fn foo(&self, val: u32) -> Result<u32>;
}
```

在 Server 端，我们实现对应的接口：

```rust
struct DemoImpl;

impl EchoService for DemoImpl {
    async fn echo(&self, r: &String) -> Result<String> {
        Ok(r.clone())
    }

    async fn foo(&self, val: u32) -> Result<u32> {
        Ok(val * 2)
    }
}
```

在 Client 端，我们希望可以直接调用 `EchoService::echo` 方法触发 RPC 请求。也就是说 `Client` 端需要有一个默认的 `EchoService` 实现，它实际上会完成序列化、发送、接收、反序列化的步骤。我们使用 Rust 的宏实现这一目的，见[代码](https://github.com/SF-Zhou/ruapc/blob/b64248314de3eacfcbf2d6ab1f3ec5f7ad6a3edf/ruapc-macro/src/lib.rs)。

### 4. 序列化

Rust 里实现序列化/反序列化还是容易的，直接用 serde 和 serde_json，例如：

```rust
use serde::{Deserialize, Serialize};

#[derive(Serialize, Deserialize)]
pub struct Request {
  pub name: String,
}
```

如此参数 `Request` 类型就有了序列化/反序列化的能力。但除了参数，我们还需要一个数据结构来定义调用本身的信息，例如要包含调用的函数名。定义 `MsgMeta`：

```rust
#[derive(Deserialize, Serialize, Debug, Default, PartialEq, Eq, Clone)]
pub struct MsgMeta {
    pub method: String,
    pub flags: MsgFlags,
}
```

其中 `method` 定义请求的方法名；`flags` 定义调用信息的一些标志位，可以暂时忽略。一个完整的请求序列化需要包含 `MsgMeta` 和参数两者的序列化结果，我们可以将这两个值放到同一个结构体一起序列化，也可以将两个其分开序列化，这里选择后者。为了在字节流中分隔开这两个值，我们还需要增加一个长度定义；为了定义序列化后消息的边界，还需要增加一个总长度的定义；为了防御一些非法请求，我们还在每个请求前增加一个 magic number。完整的消息定义如下：

1. 4 位 magic number，固定为 "RUA!"；
2. 序列化消息总长度 `total_len`，大端 `u32`
3. `MsgMeta` 总长度 `meta_len`，大端 `u32`
4. `MsgMeta` 序列化结果，长度为 `meta_len`
5. 参数序列化结果，长度为 `total_len` - `meta_len` - 4，4 为大端 `u32` 的长度

按照上述规则实现序列化的逻辑，代码参考[ socket.rs 文件](https://github.com/SF-Zhou/ruapc/blob/b64248314de3eacfcbf2d6ab1f3ec5f7ad6a3edf/ruapc/src/socket.rs#L26-L105)和 [msg.rs 文件](https://github.com/SF-Zhou/ruapc/blob/b64248314de3eacfcbf2d6ab1f3ec5f7ad6a3edf/ruapc/src/msg.rs#L40-L69)。

### 5. 连接管理

做一个简单的 Tcp 连接池，核心数据结构 `Mutex<HashMap<SocketAddr, Vec<TcpStream>>>`，尝试获取连接时如果没有则新建并尝试连接；Server 端则尝试持续从连接中解析新消息。完整代码如下：

```rust
use std::collections::HashMap;
use std::net::SocketAddr;
use std::sync::Arc;
use tokio::net::TcpStream;
use tokio::sync::Mutex;

use crate::{
    Router, Socket,
    context::{Context, SocketEndpoint},
    error::{Error, ErrorKind, Result},
};

#[derive(Default)]
pub struct SocketPool {
    map: Mutex<HashMap<SocketAddr, Vec<TcpStream>>>,
    router: Router,
}

impl SocketPool {
    #[must_use]
    pub fn create_for_server(router: Router) -> Self {
        Self {
            map: Mutex::default(),
            router,
        }
    }

    pub async fn acquire_socket(self: &Arc<Self>, addr: SocketAddr) -> Result<Socket> {
        let mut map = self.map.lock().await;
        let stream = if let Some(stream) = map.get_mut(&addr).and_then(std::vec::Vec::pop) {
            stream
        } else {
            drop(map);
            TcpStream::connect(addr)
                .await
                .map_err(|e| Error::new(ErrorKind::TcpConnectFailed, e.to_string()))?
        };

        Ok(Socket {
            socket_pool: self.clone(),
            tcp_stream: Some(stream),
            for_send: true,
        })
    }

    pub fn add_socket_for_send(self: &Arc<Self>, stream: TcpStream) {
        let this = self.clone();
        if let Ok(peer_addr) = stream.peer_addr() {
            tokio::spawn(async move {
                let mut map = this.map.lock().await;
                map.entry(peer_addr).or_default().push(stream);
            });
        }
    }

    pub fn add_socket_for_recv(self: &Arc<Self>, stream: TcpStream) {
        let socket = Socket {
            socket_pool: self.clone(),
            tcp_stream: Some(stream),
            for_send: false,
        };
        let this = self.clone();
        tokio::spawn(async move {
            let _ = this.handle_request(socket).await;
        });
    }

    async fn handle_request(self: &Arc<Self>, mut socket: Socket) -> Result<()> {
        let msg = socket.recv().await?;
        let ctx = Context {
            socket_pool: self.clone(),
            endpoint: SocketEndpoint::Connected(socket),
        };
        self.router.dispatch(ctx, msg);
        Ok(())
    }
}
```

### 6. 组装一下

完整代码参见[该链接](https://github.com/SF-Zhou/ruapc/blob/b64248314de3eacfcbf2d6ab1f3ec5f7ad6a3edf)。

