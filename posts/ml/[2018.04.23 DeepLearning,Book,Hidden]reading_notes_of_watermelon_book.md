# 《机器学习》西瓜书阅读笔记

## 第 1 章 绪论

### 基本术语

`机器学习`：在计算机上从`数据`（data）中产生`模型`（model）的算法，即`学习算法`（learning algorithm）。

A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.

一般地，令 $D = \left \{ \vec {x}_1, \vec {x}_2, \cdots, \vec {x}_m \right \}$ 表示包含 $m$ 个`样本`（sample）的数据集，每个示例由 $d$ 个`属性`（attribute）描述，则每个样本 $\vec x_i = \left \{x_{i1}; x_{i2}; \cdots; x_{id} \right \}$ 是 $d$ 维样本空间 $\mathcal{X}$ 中的一个向量，$\vec x_i \in \mathcal{X}$，其中 $x_{ij}$ 是 $\vec x_i$ 在第 $j$ 个属性上的取值，$d$ 称为样本 $\vec x_i$ 的`维数`（dimensionality）。

属性张成的空间称为`样本空间`（sample space），每个样本都可在这个空间中找到唯一的坐标位置，因此也把一个样本称为一个`特征向量`（feature vector）。

从数据中学得模型的过程称之为`学习`（learning）或`训练`（training），学得模型适用于新样本的能力称为`泛化`（generalization）能力。

### 假设空间

`归纳`（induction）与`演绎`（deduction）是科学推理的两大基本手段。前者是从特殊到一般的泛化（generalization）过程，后者是从一般到特殊的特化（specialization）过程。从样例中学习是一个归纳的过程，亦称`归纳学习`（inductive learning）。

狭义的归纳学习是从数据中学得`概念`（concept），最基本的概念学习是布尔概念学习。可以把学习的过程看作一个在所有`假设`（hypothesis）组成的空间中进行搜索的过程，搜索目标是找到与训练集`匹配`（fit）的假设。

假设的表示一旦确定，`假设空间`（hypothesis space）及其规模大小就确定了。现实问题中通常面临很大的假设空间，但样本训练集是有限的，因此可能有多个假设与训练集一致，即存在一个与训练集一致的假设集合，称之为`版本空间`（version space）。

### 归纳偏好

机器学习算法在学习过程中对某种类型假设的偏好，称为`归纳偏好`（inductive bias）。归纳偏好可看作是学习算法在庞大的假设空间中对假设进行选择的价值观。

`奥卡姆剃刀`（Occam's Razor）是自然科学研究中常用的原则，即若存在多个假设与观察一致，则选最简单的那个。如无必要，勿增实体。

但奥卡姆剃刀原则并不平凡，“简单”的评价标准无法量化。事实上归纳偏好对应了学习算法本身所做出的关于“什么样的模型更好”的假设。`没有免费的午餐定理`（No Free Lunch Theorem，NFL）证明了在真实目标函数 $f$ 均匀分布的情况下，所有学习算法学得的模型期望性能是一致的。

脱离实际问题，空谈“什么学习算法更好”毫无意义。

## 第 2 章 模型评估与选择

### 经验误差与过拟合

学习器的实际输出与样本的真实输出之间的差异称为`误差`（error），训练集上的误差称为`训练误差`（training error），新样本上的误差称为`泛化误差`（generalization error）。

为了使泛化误差最小化，应该从训练样本中尽可能学出适用于所有潜在样本的“普遍规律”。而将训练样本的特点当作了所有潜在样本的一般性质，导致泛化性能下降的现象，称为`过拟合`（overfitting），相对地没有充分习得训练样本的一般性质的现象，称为`欠拟合`（underfitting）。

现实任务中，存在多种学习算法、不同参数配置，产生不同的模型，需要选择其中合适的模型，该问题称为`模型选择`（model selection）问题。理想状态下使用泛化误差作为模型选择的评价标准，但泛化误差无法直接获得。

### 评估方法

通常使用`测试集`（testing set）来测试学习器对新样本的判别能力，以测试集上的`测试误差`（testing error）作为泛化误差的近似。通常假设测试样本是从样本真实分布中独立同分布采样而得。

对于包含 $m$ 个样本的数据集 $D = \left \{ (\vec x_1, y_1), (\vec x_2, y_2), \cdots, (\vec x_m, y_m) \right \}$，需要将其分解为训练集 $S$、验证集 $V$ 和测试集 $T$，常用的方法有留出法、交叉验证法和`自助法`（bootstrapping）。

自助法即从数据集中进行 $m$ 次可重复采样，可以选出约 36.8% 的样本作为测试集，在数据集较小时较为有效。

机器学习常涉及两类参数：一是算法的参数，称为`超参数`（hyper parameter），一是模型的参数。对超参数进行设定调优的过程称为`调参`（parameter tuning）。通常使用验证集进行模型选择和调参，使用测试集评估模型的泛化能力。

### 性能度量

性能度量（performance measure），即为模型泛化能力的评价标准。给定数据集 $D = \left \{ (\vec x_1, y_1), (\vec x_2, y_2), \cdots, (\vec x_m, y_m) \right \}$，其中 $y_i$ 是样本 $\vec x_i$ 的真实标记。

回归任务常用的性能度量是`均方误差`（mean squared error）：

$$
E(f; \mathcal{D}) = \int_{\vec x \sim \mathcal D} (f(\vec x) - y)^2 p(\vec {x}) d\vec x
$$

分类任务常用的性能度量较多，常用的错误率：

$$
E(f; \mathcal{D}) = \int_{\vec x \sim \mathcal D} \mathbb I(f(\vec x) \neq y) p(\vec {x}) d\vec x
$$

`准确率`（percision）和`召回率`（recall）：

$$
\begin{aligned}
P &= \frac {TP} {TP + FP} \\
R &= \frac {TP} {TP + FN}
\end{aligned}
$$

|          | 预测正例 | 预测负例 |
| :------: | :------: | :------: |
| 真实正例 |    TP    |    FN    |
| 真实负例 |    FP    |    TN    |

准确率和召回率不可得兼。以准确率作为纵轴、召回率作为横轴，可以得到`P-R曲线`，曲线中“准确率=召回率”的点成为`平衡点`（Break-Even Point）。

准确率和召回率的`调和平均`（harmonic mean）称为`F1`度量：

$$
\begin{aligned}
\frac {1} {F1} &= \frac {1} {2} (\frac {1} {P} + \frac {1} {R}) \\
F1 &= \frac {2PR} {P + R}
\end{aligned}
$$

由多组混淆矩阵计算多组准确率和召回率，再求平均值，可得`宏准确率`（macro-P）和`宏召回率`（macro-R）；将多组混淆矩阵求平均值，再求准确率和召回率，可得`微准确率`（micro-P）和`微召回率`（micro-R）。

`ROC` 全称受试者工作特征（Receiver Operating Characteristic），该曲线以`真正例率`（True Positive Rate）为纵轴，以`假正例率`（False Positive Rate）为横轴：

$$
\begin{aligned}
TPR &= \frac {TP} {TP + FN} \\
FPR & = \frac {FP} {TN + FP}
\end{aligned}
$$

ROC 曲线下的面积称为`AUC`（Area Under ROC Curve），通常使用 AUC 作为ROC 曲线优劣的判断依据。

不同类型的错误所造成的后果不同，为权衡不同类型错误所造成的不同损失，可为错误赋予`非均等代价`（unequal cost）。令 $D^+$ 与 $D^-$ 代表数据集 $D$ 中的正例子集和反例子集，则`代价敏感`（cost-sensitive）错误率为：

$$
E(f; D; cost) = \frac {1} {m}
\left (
\sum_{\vec x_i \in D^+} \mathbb I (f(\vec x_i) \neq y_i) cost_{01}
+ \sum_{\vec x_i \in D^-} \mathbb I (f(\vec x_i) \neq y_i) cost_{10}
\right )
$$

### 偏差与方差

`偏差-方差分解`（bias-veriance decomposition）是解释学习算法泛化性能的一种重要工具。对测试样本 $\vec x$，令 $y_D$ 为 $\vec x$ 在数据集中的标记，$y$ 为 $\vec x$ 的真实标记，$f(\vec x; D)$ 为训练集 $D$ 上学的模型 $f$ 在 $\vec x$ 上的预测输出。以回归任务为例，学习算法的期望预测为：

$$
\bar f(\vec x) = \mathbb E_D \left [ f(\vec x; D) \right ]
$$

期望输出与真实标记的差别称为`偏差`（bias）：

$$
bias^2(\vec x) = \left ( \bar f(\vec x) - y \right )^2
$$

使用样本数相同的不同训练集产生的`方差`（variance）为：

$$
var(\vec x) = \mathbb E_D \left [ \left (f(\vec x; D) - \bar f(\vec x) \right )^2 \right ]
$$

噪声为：

$$
\varepsilon ^2 = \mathbb E_D \left [ (y_D - y)^2 \right ]
$$

假定噪声的期望为零，可得：

$$
E(f; D) = bias^2(\vec x) + var(\vec x) + \varepsilon ^2
$$

即泛化误差可以分解为偏差、方差和噪声之和。偏差和方差间存在`偏差-方差窘境`（bias-variance dilemma），当学习算法训练不足时，学习器的拟合能力不够强，偏差主导了泛化错误率；当训练程度加深后，学习器的拟合能力足够，方差主导了泛化错误率。

## 第 3 章 线性模型

### 基本形式

给定由 $d$ 个属性描述的示例 $\boldsymbol x = (x_1; x_2; \cdots; x_d)$，其中 $x_i$ 是 $\boldsymbol x$ 在第 $i$ 个属性上的取值，`线性模型`（linear model）试图学得一个通过属性的线性组合来进行预测的函数：

$$
f(\boldsymbol x) = \boldsymbol w^T \boldsymbol x + b
$$

其中 $\boldsymbol w = (w_1; w_2; \cdots; w_d)$。

线性模型形式简单，易于建模，且 $\boldsymbol w$ 直观表达了各属性在预测中的重要性，因此线性模型有很好的 `可解释性`（comprehensibility）。

在线性模型的基础上可通过引入层级结构或高维映射而得到更为强大的`非线性模型`（nonlinear model）。

### 线性回归

给定数据集 $D = \{(\boldsymbol x_1, y_1), (\boldsymbol x_2, y_2), \cdots , (\boldsymbol x_m, y_m)\}$，其中 $\boldsymbol x_i = (x_{i1}, x_{i2}, \cdots, x_{id})$，$y \in \mathbb R$，`线性回归`（linear regression）试图学得一个线性模型以尽可能准确地预测实际输出标记。

考虑最简单的单属性情形，$D = \left \{ (x_i, y_i) \right \}_{i=1}^m$，线性回归试图学得

$$
f(x_i) = wx_i+b
$$

以使得 $f(x_i) \simeq y_i$。使用均方误差作为衡量 $f(x)$ 与 $y$ 之间差别的性能度量：

$$
E_{(w, b)} = \sum_{i=1}^{m} {\left (f(x_i) - y_i \right )^2}
$$

则：

$$
(w^*, b^*) = \underset {(w, b)} {\arg \min} E_{(w, b)}
$$

均方误差有非常好的几何意义，它对应了 `欧氏距离`（Euclidean distance）。基于均方误差最小化来进行模型求解的方法称为 `最小二乘法`（least square method）。在线性回归中，最小二乘法试图找到一条直线，使得所有样本到直线上的欧氏距离之和最小。

$E_{(w, b)}$ 是关于 $w$ 和 $b$ 的凸函数。对于区间 $[a, b]$ 上定义的函数 $f$，若它区间中任意两点 $x_1$ 和 $x_2$ 均有 $f(\frac {x_1 + x_2} {2}) \le \frac {f(x_1) + f(x_2)} {2}$，则称 $f$ 为区间 $[a, b]$ 上的凸函数。对实数集上的函数，可以通过求二阶导数的方式来判断，二阶导数在区间上非负则称为凸函数。

求解 $w$ 和 $b$ 使均方误差最小化的过程，称为线性回归模型的最小二乘 `参数估计`（parameter estimation）。将 $E_{(w, b)}$ 分别对 $w$ 和 $b$ 求导，得到：

$$
\begin {aligned}
\frac {\partial E_{(w, b)}} {\partial w} &= 2 \left ( w \sum_{i=1}^{m} x_i^2 - \sum_{i=1}^{m} (y_i - b)x_i \right ) \\
\frac {\partial E_{(w, b)}} {\partial b} &= 2 \left ( mb - \sum_{i=1}^{m} (y_i - wx_i) \right )
\end {aligned}
$$

对 $w$ 和 $b$ 的偏导置零可得到 $w$ 和 $b$ 最优解的 `闭式解`（closed-form solution）：

$$
\begin {aligned}
w &= \frac {\sum_{i=1}^{m}x_i y_i - m \bar x \bar y} {\sum_{i=1}^{m} {x_i^2} - m \bar x^2} \\
b &= \bar y - w \bar x
\end {aligned}
$$

更一般的情形，给定数据集 $D = \left \{ (\boldsymbol x_i, y_i) \right \}_{i=1}^m$，其中 $\boldsymbol x_i = (x_{i1}, x_{i2}, \cdots, x_{id})$，$y \in \mathbb R$，线性回归试图学得：

$$
f(\boldsymbol x_i) = \boldsymbol w^T \boldsymbol x_i+b
$$

使得 $f(\boldsymbol x_i) \simeq y_i$。这称为 `多变量线性回归`（multivariate linear regression）。

[未完待续]

### 参考文献

1. 周志华. "机器学习." 清华大学出版社，北京.

