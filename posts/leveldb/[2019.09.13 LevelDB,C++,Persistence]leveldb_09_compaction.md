# LevelDB æºç åˆ†æã€Œä¹ã€Compactionã€

*ç¥å¤§å®¶ä¸­ç§‹å¿«ä¹ï¼*

LevelDB æºç åˆ†æç³»åˆ—ä¹Ÿæ­¥å…¥å°¾å£°ï¼Œæœ¬ç¯‡å°†åˆ†æ LevelDB ä¸­è‡³å…³é‡è¦çš„ Compaction è¿‡ç¨‹ï¼Œä¾ç„¶ä»ä»£ç çš„è§’åº¦å‡ºå‘ã€‚å»ºè®®å¤§å®¶åŒæ—¶é˜…è¯»[å‚è€ƒæ–‡çŒ® 1](https://leveldb-handbook.readthedocs.io/zh/latest/compaction.html) äº†è§£ Compaction çš„ä½œç”¨å’Œè¿‡ç¨‹çš„æè¿°ã€‚

### 1. è§¦å‘ Compaction

[æœ¬ç³»åˆ—ç¬¬ä¸‰ç¯‡](/leveldb/leveldb_05_sorted_table.html)ä¸­æè¿°äº†å†…å­˜æ•°æ®åº“è½¬ä¸º Sorted Table çš„è¿‡ç¨‹ï¼Œå…¶ä¸­ä¼šæ‰§è¡Œ `DBImpl::BackgroundCompaction` è¿™ä¸€åå°ä»»åŠ¡ï¼š

```c++
void DBImpl::BackgroundCompaction() {
  mutex_.AssertHeld();

  if (imm_ != nullptr) {
    CompactMemTable();
    return;
  }

  Compaction* c;
  bool is_manual = (manual_compaction_ != nullptr);
  InternalKey manual_end;
  if (is_manual) {
    ...
  } else {
    c = versions_->PickCompaction();
  }

  ...
}
```

ç°åœ¨å‡è®¾ `imm_` ä¸ºç©ºï¼Œå¹¶ä¸”ä¸è€ƒè™‘æ‰‹åŠ¨ Compactionï¼Œé‚£ä¹ˆè¿™é‡Œä¼šæ‰§è¡Œ `versions_->PickCompaction` å»é€‰æ‹©ä¸€ä¸ª Compactionï¼Œå…¶å®ç°ä½äº [`db/version_set.cc`](https://github.com/google/leveldb/blob/master/db/version_set.cc)ï¼š

```c++
Compaction* VersionSet::PickCompaction() {
  Compaction* c;
  int level;

  // We prefer compactions triggered by too much data in a level over
  // the compactions triggered by seeks.
  const bool size_compaction = (current_->compaction_score_ >= 1);
  const bool seek_compaction = (current_->file_to_compact_ != nullptr);
  ...
}
```

è¿™é‡Œä¼šæœ‰ä¸¤ç§éœ€è¦ Compaction çš„æƒ…å†µï¼Œä¸€ç§æ˜¯æŸä¸€ Level çš„åˆ†æ•°è¶…è¿‡äº† 1ï¼Œä¸€ç§æ˜¯æŸä¸€ä¸ªæ–‡ä»¶çš„æ— æ•ˆæŸ¥è¯¢æ¬¡æ•°è¶…è¿‡é˜ˆå€¼ã€‚åˆ†æ•°çš„è®¡ç®—ä½äºç‰ˆæœ¬æ›´æ–°ä¹‹åçš„ `VersionSet::Finalize`ï¼š

```c++
void VersionSet::Finalize(Version* v) {
  // Precomputed best level for next compaction
  int best_level = -1;
  double best_score = -1;

  for (int level = 0; level < config::kNumLevels - 1; level++) {
    double score;
    if (level == 0) {
      // We treat level-0 specially by bounding the number of files
      // instead of number of bytes for two reasons:
      //
      // (1) With larger write-buffer sizes, it is nice not to do too
      // many level-0 compactions.
      //
      // (2) The files in level-0 are merged on every read and
      // therefore we wish to avoid too many files when the individual
      // file size is small (perhaps because of a small write-buffer
      // setting, or very high compression ratios, or lots of
      // overwrites/deletions).
      score = v->files_[level].size() /
              static_cast<double>(config::kL0_CompactionTrigger);
    } else {
      // Compute the ratio of current size to size limit.
      const uint64_t level_bytes = TotalFileSize(v->files_[level]);
      score =
          static_cast<double>(level_bytes) / MaxBytesForLevel(options_, level);
    }

    if (score > best_score) {
      best_level = level;
      best_score = score;
    }
  }

  v->compaction_level_ = best_level;
  v->compaction_score_ = best_score;
}
```

å¯¹äº 0 å±‚æ–‡ä»¶ï¼Œå½“æ–‡ä»¶æ•°é‡è¶…è¿‡é˜ˆå€¼ï¼ˆé»˜è®¤ 4ï¼‰æ—¶è§¦å‘ Compactionï¼›å¯¹äºå…¶ä»–å±‚çš„æ–‡ä»¶ï¼Œå½“æ–‡ä»¶çš„æ€»å¤§å°è¶…è¿‡é˜ˆå€¼ï¼ˆé»˜è®¤ $10^{l}$MBï¼‰æ—¶è§¦å‘ Compactionã€‚è€Œä¸€ä¸ªæ–‡ä»¶çš„çš„æŸ¥è¯¢æ¬¡æ•°é˜ˆå€¼å®šä¹‰äº `VersionSet::Builder::Apply`ï¼š

```c++
// We arrange to automatically compact this file after
// a certain number of seeks.  Let's assume:
//   (1) One seek costs 10ms
//   (2) Writing or reading 1MB costs 10ms (100MB/s)
//   (3) A compaction of 1MB does 25MB of IO:
//         1MB read from this level
//         10-12MB read from next level (boundaries may be misaligned)
//         10-12MB written to next level
// This implies that 25 seeks cost the same as the compaction
// of 1MB of data.  I.e., one seek costs approximately the
// same as the compaction of 40KB of data.  We are a little
// conservative and allow approximately one seek for every 16KB
// of data before triggering a compaction.
f->allowed_seeks = static_cast<int>((f->file_size / 16384U));
if (f->allowed_seeks < 100) f->allowed_seeks = 100;
```

è‹±æ–‡æ³¨é‡Šå†™å¾—ååˆ†è¯¦ç»†ã€‚é¦–å…ˆå‡è®¾ï¼š

1. ä¸€æ¬¡æŸ¥è¯¢è€—æ—¶ 10msï¼›
2. è¯»/å†™ 1MB è€—æ—¶ 10ms ï¼ˆå‡è®¾é€Ÿåº¦ 100MB/sï¼‰ï¼›
3. 1MB çš„ Compaction éœ€è¦åš 25 MB çš„ IO
   1. æœ¬å±‚è¯» 1MBï¼›
   2. ä¸‹ä¸€å±‚è¯» 10-12 MB
   3. Compaction åå†™ 10-12 MB

æ•´ä½“æ¥çœ‹ï¼Œ1MB çš„æ•°æ®åš 25 æ¬¡æŸ¥è¯¢å’Œ Compaction çš„æ—¶é—´å·®ä¸å¤šï¼Œ1 æ¬¡æŸ¥è¯¢å°±ç›¸å½“äºåš 40KB æ•°æ®çš„ Compactionã€‚LevelDB å°†å…¶è®¾ä¸ºæ›´ä¿å®ˆçš„ 16KBï¼Œè¿›è€Œä¸€ä¸ªæ–‡ä»¶çš„æŸ¥è¯¢æ¬¡æ•°é˜ˆå€¼è®¾å®šä¸º `FileSize / 16KB`ã€‚å½“ä¸€æ¬¡æŸ¥è¯¢ä¸­è¯»å–äº†å¤šä¸ªæ–‡ä»¶ï¼Œåˆ™å°†ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„æŸ¥è¯¢æ¬¡æ•° +1ï¼Œç›´åˆ°å…¶è¶…è¿‡é˜ˆå€¼ã€è§¦å‘ Compactionã€‚ç»§ç»­çœ‹ `VersionSet::PickCompaction`ï¼š

```c++
Compaction* VersionSet::PickCompaction() {
  ...

  if (size_compaction) {
    level = current_->compaction_level_;
    assert(level >= 0);
    assert(level + 1 < config::kNumLevels);
    c = new Compaction(options_, level);

    // Pick the first file that comes after compact_pointer_[level]
    for (size_t i = 0; i < current_->files_[level].size(); i++) {
      FileMetaData* f = current_->files_[level][i];
      if (compact_pointer_[level].empty() ||
          icmp_.Compare(f->largest.Encode(), compact_pointer_[level]) > 0) {
        c->inputs_[0].push_back(f);
        break;
      }
    }
    if (c->inputs_[0].empty()) {
      // Wrap-around to the beginning of the key space
      c->inputs_[0].push_back(current_->files_[level][0]);
    }
  } else if (seek_compaction) {
    level = current_->file_to_compact_level_;
    c = new Compaction(options_, level);
    c->inputs_[0].push_back(current_->file_to_compact_);
  } else {
    return nullptr;
  }

  c->input_version_ = current_;
  c->input_version_->Ref();

  // Files in level 0 may overlap each other, so pick up all overlapping ones
  if (level == 0) {
    InternalKey smallest, largest;
    GetRange(c->inputs_[0], &smallest, &largest);
    // Note that the next call will discard the file we placed in
    // c->inputs_[0] earlier and replace it with an overlapping set
    // which will include the picked file.
    current_->GetOverlappingInputs(0, &smallest, &largest, &c->inputs_[0]);
    assert(!c->inputs_[0].empty());
  }

  SetupOtherInputs(c);

  return c;
}
```

å¯¹äºæ•°æ®å¤§å°è§¦å‘çš„ Compactionï¼Œä¼šé€‰å– `compact_pointer_` åçš„ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸º Compaction å¯¹è±¡ï¼Œå³æœ¬å±‚ä¸Šä¸€æ¬¡ Compaction åŒºé—´ä¹‹åçš„æ–‡ä»¶ï¼›è€ŒæŸ¥è¯¢æ¬¡æ•°è§¦å‘çš„ Compaction å…¶æœ¬èº«å¯¹åº”ä¸€ä¸ªæ–‡ä»¶ã€‚å¯¹äº 0 å±‚æ–‡ä»¶ï¼Œå› ä¸ºä¹‹é—´å­˜åœ¨ Overlapï¼Œéœ€è¦å°†å­˜åœ¨é‡å çš„æ–‡ä»¶éƒ½åŠ å…¥ Compaction é›†åˆé‡Œã€‚è‡³æ­¤æœ¬å±‚çš„æ–‡ä»¶é€‰æ‹©å®Œæ¯•ã€‚

### 2. æ‰©å¤§ Compaction æ–‡ä»¶é›†åˆ

`VersionSet::PickCompaction` éšåæ‰§è¡Œ `SetupOtherInputs` ä»¥æ‰©å¤§ Compaction æ–‡ä»¶é›†åˆï¼š

```c++
// Finds the largest key in a vector of files. Returns true if files it not
// empty.
bool FindLargestKey(const InternalKeyComparator& icmp,
                    const std::vector<FileMetaData*>& files,
                    InternalKey* largest_key) {
  if (files.empty()) {
    return false;
  }
  *largest_key = files[0]->largest;
  for (size_t i = 1; i < files.size(); ++i) {
    FileMetaData* f = files[i];
    if (icmp.Compare(f->largest, *largest_key) > 0) {
      *largest_key = f->largest;
    }
  }
  return true;
}

// Finds minimum file b2=(l2, u2) in level file for which l2 > u1 and
// user_key(l2) = user_key(u1)
FileMetaData* FindSmallestBoundaryFile(
    const InternalKeyComparator& icmp,
    const std::vector<FileMetaData*>& level_files,
    const InternalKey& largest_key) {
  const Comparator* user_cmp = icmp.user_comparator();
  FileMetaData* smallest_boundary_file = nullptr;
  for (size_t i = 0; i < level_files.size(); ++i) {
    FileMetaData* f = level_files[i];
    if (icmp.Compare(f->smallest, largest_key) > 0 &&
        user_cmp->Compare(f->smallest.user_key(), largest_key.user_key()) ==
            0) {
      if (smallest_boundary_file == nullptr ||
          icmp.Compare(f->smallest, smallest_boundary_file->smallest) < 0) {
        smallest_boundary_file = f;
      }
    }
  }
  return smallest_boundary_file;
}

// Extracts the largest file b1 from |compaction_files| and then searches for a
// b2 in |level_files| for which user_key(u1) = user_key(l2). If it finds such a
// file b2 (known as a boundary file) it adds it to |compaction_files| and then
// searches again using this new upper bound.
//
// If there are two blocks, b1=(l1, u1) and b2=(l2, u2) and
// user_key(u1) = user_key(l2), and if we compact b1 but not b2 then a
// subsequent get operation will yield an incorrect result because it will
// return the record from b2 in level i rather than from b1 because it searches
// level by level for records matching the supplied user key.
//
// parameters:
//   in     level_files:      List of files to search for boundary files.
//   in/out compaction_files: List of files to extend by adding boundary files.
void AddBoundaryInputs(const InternalKeyComparator& icmp,
                       const std::vector<FileMetaData*>& level_files,
                       std::vector<FileMetaData*>* compaction_files) {
  InternalKey largest_key;

  // Quick return if compaction_files is empty.
  if (!FindLargestKey(icmp, *compaction_files, &largest_key)) {
    return;
  }

  bool continue_searching = true;
  while (continue_searching) {
    FileMetaData* smallest_boundary_file =
        FindSmallestBoundaryFile(icmp, level_files, largest_key);

    // If a boundary file was found advance largest_key, otherwise we're done.
    if (smallest_boundary_file != NULL) {
      compaction_files->push_back(smallest_boundary_file);
      largest_key = smallest_boundary_file->largest;
    } else {
      continue_searching = false;
    }
  }
}

void VersionSet::SetupOtherInputs(Compaction* c) {
  const int level = c->level();
  InternalKey smallest, largest;

  AddBoundaryInputs(icmp_, current_->files_[level], &c->inputs_[0]);
  ...
}
```

é¦–å…ˆæ‰§è¡Œçš„æ˜¯ `AddBoundaryInputs`ã€‚å…¶è‹±æ–‡æ³¨é‡Šä¸­è§£é‡Šåœ°éå¸¸è¯¦ç»†ï¼šå½“ Compaction çš„èŒƒå›´ä¸º $[l1, u1]$ æ—¶ï¼Œè¯¥èŒƒå›´çš„æ•°æ®å°†ä¼šè¢«ç§»åŠ¨åˆ° Level+1ã€‚å¦‚æœå½“å‰ Level å­˜åœ¨æ–‡ä»¶ $[l2, u2]$ï¼Œå¹¶ä¸” `user_key(u1) = user_key(l2)`ï¼Œé‚£ä¹ˆä¸‹ä¸€æ¬¡æŸ¥è¯¢ `user_key(u1)` æ—¶ä¼šåœ¨ Level å±‚æå‰è¿”å›æ—§çš„æ•°æ®ï¼æ•…éœ€è¦å°†å—å½±å“çš„æ–‡ä»¶å…¨éƒ¨åŠ åˆ° Compaction æ–‡ä»¶èŒƒå›´ä¸­ã€‚ç»§ç»­çœ‹ `VersionSet::SetupOtherInputs`ï¼š

```c++
void VersionSet::SetupOtherInputs(Compaction* c) {
  ...
  GetRange(c->inputs_[0], &smallest, &largest);

  current_->GetOverlappingInputs(level + 1, &smallest, &largest,
                                 &c->inputs_[1]);

  // Get entire range covered by compaction
  InternalKey all_start, all_limit;
  GetRange2(c->inputs_[0], c->inputs_[1], &all_start, &all_limit);

  // See if we can grow the number of inputs in "level" without
  // changing the number of "level+1" files we pick up.
  if (!c->inputs_[1].empty()) {
    std::vector<FileMetaData*> expanded0;
    current_->GetOverlappingInputs(level, &all_start, &all_limit, &expanded0);
    AddBoundaryInputs(icmp_, current_->files_[level], &expanded0);
    const int64_t inputs0_size = TotalFileSize(c->inputs_[0]);
    const int64_t inputs1_size = TotalFileSize(c->inputs_[1]);
    const int64_t expanded0_size = TotalFileSize(expanded0);
    if (expanded0.size() > c->inputs_[0].size() &&
        inputs1_size + expanded0_size <
            ExpandedCompactionByteSizeLimit(options_)) {
      InternalKey new_start, new_limit;
      GetRange(expanded0, &new_start, &new_limit);
      std::vector<FileMetaData*> expanded1;
      current_->GetOverlappingInputs(level + 1, &new_start, &new_limit,
                                     &expanded1);
      if (expanded1.size() == c->inputs_[1].size()) {
        Log(options_->info_log,
            "Expanding@%d %d+%d (%ld+%ld bytes) to %d+%d (%ld+%ld bytes)\n",
            level, int(c->inputs_[0].size()), int(c->inputs_[1].size()),
            long(inputs0_size), long(inputs1_size), int(expanded0.size()),
            int(expanded1.size()), long(expanded0_size), long(inputs1_size));
        smallest = new_start;
        largest = new_limit;
        c->inputs_[0] = expanded0;
        c->inputs_[1] = expanded1;
        GetRange2(c->inputs_[0], c->inputs_[1], &all_start, &all_limit);
      }
    }
  }

  // Compute the set of grandparent files that overlap this compaction
  // (parent == level+1; grandparent == level+2)
  if (level + 2 < config::kNumLevels) {
    current_->GetOverlappingInputs(level + 2, &all_start, &all_limit,
                                   &c->grandparents_);
  }

  // Update the place where we will do the next compaction for this level.
  // We update this immediately instead of waiting for the VersionEdit
  // to be applied so that if the compaction fails, we will try a different
  // key range next time.
  compact_pointer_[level] = largest.Encode().ToString();
  c->edit_.SetCompactPointer(level, largest);
}
```

é¦–å…ˆåœ¨ Level+1 å±‚å°†æ‰€æœ‰å­˜åœ¨é‡å çš„æ–‡ä»¶åŠ å…¥ Compaction æ–‡ä»¶é›†åˆé‡Œï¼Œæ›´æ–° Compaction çš„åŒºé—´ `[all_start, all_limit]`ã€‚å†å›è¿‡å¤´æ¥ä½¿ç”¨æ–°åŒºé—´è·å¾— Level å±‚é‡å çš„æ–‡ä»¶ `expanded0`ï¼Œå¦‚æœæ–°çš„æ•°æ®å¤§å°åœ¨é˜ˆå€¼ä»¥å†…ä¸”ä¸ä¼šæ”¹å˜ Level+1 å±‚é€‰æ‹©çš„æ–‡ä»¶ï¼Œé‚£ä¹ˆåˆ™å°† Level å±‚çš„æ–‡ä»¶é›†åˆæ›´æ–°ä¸º `expanded0`ã€‚æœ€åå°†å½“å‰ Level çš„ `compact_pointer_` è®¾ä¸ºå½“å‰ Compaction çš„æœ€å¤§é”®ã€‚è‡³æ­¤æ‰©å¤§ Compaction æ–‡ä»¶é›†åˆç»“æŸï¼Œ`VersionSet::PickCompaction` ä¹Ÿè¿”å›äº† Compaction å¯¹è±¡ã€‚

### 3. æ‰§è¡Œ Compaction

å›åˆ° `DBImpl::BackgroundCompaction`ï¼š

```c++
struct DBImpl::CompactionState {
  // Files produced by compaction
  struct Output {
    uint64_t number;
    uint64_t file_size;
    InternalKey smallest, largest;
  };

  Output* current_output() { return &outputs[outputs.size() - 1]; }

  explicit CompactionState(Compaction* c)
      : compaction(c),
        smallest_snapshot(0),
        outfile(nullptr),
        builder(nullptr),
        total_bytes(0) {}

  Compaction* const compaction;

  // Sequence numbers < smallest_snapshot are not significant since we
  // will never have to service a snapshot below smallest_snapshot.
  // Therefore if we have seen a sequence number S <= smallest_snapshot,
  // we can drop all entries for the same key with sequence numbers < S.
  SequenceNumber smallest_snapshot;

  std::vector<Output> outputs;

  // State kept for output being generated
  WritableFile* outfile;
  TableBuilder* builder;

  uint64_t total_bytes;
};

void DBImpl::BackgroundCompaction() {
  ...

  Status status;
  if (c == nullptr) {
    // Nothing to do
  } else if (!is_manual && c->IsTrivialMove()) {
    ...
  } else {
    CompactionState* compact = new CompactionState(c);
    status = DoCompactionWork(compact);
    if (!status.ok()) {
      RecordBackgroundError(status);
    }
    CleanupCompaction(compact);
    c->ReleaseInputs();
    DeleteObsoleteFiles();
  }
  delete c;

  if (status.ok()) {
    // Done
  } else if (shutting_down_.load(std::memory_order_acquire)) {
    // Ignore compaction errors found during shutting down
  } else {
    Log(options_.info_log, "Compaction error: %s", status.ToString().c_str());
  }

  if (is_manual) {
    ...
  }
}
```

ä¸è€ƒè™‘æ‰‹åŠ¨æ¨¡å¼å’Œ TrivialMoveï¼Œæ¥ä¸‹æ¥ä¼šæ ¹æ® `Compaction` å¯¹è±¡æ„å»º `CompactionState`ï¼Œå¹¶æ‰§è¡Œ `DBImpl::DoCompactionWork`ï¼š

```c++
Status DBImpl::DoCompactionWork(CompactionState* compact) {
  const uint64_t start_micros = env_->NowMicros();
  int64_t imm_micros = 0;  // Micros spent doing imm_ compactions

  Log(options_.info_log, "Compacting %d@%d + %d@%d files",
      compact->compaction->num_input_files(0), compact->compaction->level(),
      compact->compaction->num_input_files(1),
      compact->compaction->level() + 1);

  assert(versions_->NumLevelFiles(compact->compaction->level()) > 0);
  assert(compact->builder == nullptr);
  assert(compact->outfile == nullptr);
  if (snapshots_.empty()) {
    compact->smallest_snapshot = versions_->LastSequence();
  } else {
    compact->smallest_snapshot = snapshots_.oldest()->sequence_number();
  }

  Iterator* input = versions_->MakeInputIterator(compact->compaction);
  ...
}
```

`compact->smallest_snapshot` æ˜¯ä¸ºäº†è®©å½“å‰çš„ Snapshot çš„æ•°æ®åœ¨ Compaction è¿‡ç¨‹ä¸­ä¸ä¸¢å¤±ã€‚`versions_->MakeInputIterator` è¿”å› Compaction æ–‡ä»¶é›†åˆçš„åˆå¹¶è¿­ä»£å™¨ï¼š

```c++
Iterator* VersionSet::MakeInputIterator(Compaction* c) {
  ReadOptions options;
  options.verify_checksums = options_->paranoid_checks;
  options.fill_cache = false;

  // Level-0 files have to be merged together.  For other levels,
  // we will make a concatenating iterator per level.
  // TODO(opt): use concatenating iterator for level-0 if there is no overlap
  const int space = (c->level() == 0 ? c->inputs_[0].size() + 1 : 2);
  Iterator** list = new Iterator*[space];
  int num = 0;
  for (int which = 0; which < 2; which++) {
    if (!c->inputs_[which].empty()) {
      if (c->level() + which == 0) {
        const std::vector<FileMetaData*>& files = c->inputs_[which];
        for (size_t i = 0; i < files.size(); i++) {
          list[num++] = table_cache_->NewIterator(options, files[i]->number,
                                                  files[i]->file_size);
        }
      } else {
        // Create concatenating iterator for the files from this level
        list[num++] = NewTwoLevelIterator(
            new Version::LevelFileNumIterator(icmp_, &c->inputs_[which]),
            &GetFileIterator, table_cache_, options);
      }
    }
  }
  assert(num <= space);
  Iterator* result = NewMergingIterator(&icmp_, list, num);
  delete[] list;
  return result;
}
```

ç»§ç»­çœ‹ `DBImpl::DoCompactionWork`ï¼š

```c++
Status DBImpl::DoCompactionWork(CompactionState* compact) {
  ...
  // Release mutex while we're actually doing the compaction work
  mutex_.Unlock();

  input->SeekToFirst();
  Status status;
  ParsedInternalKey ikey;
  std::string current_user_key;
  bool has_current_user_key = false;
  SequenceNumber last_sequence_for_key = kMaxSequenceNumber;
  while (input->Valid() && !shutting_down_.load(std::memory_order_acquire)) {
    // Prioritize immutable compaction work
    if (has_imm_.load(std::memory_order_relaxed)) {
      const uint64_t imm_start = env_->NowMicros();
      mutex_.Lock();
      if (imm_ != nullptr) {
        CompactMemTable();
        // Wake up MakeRoomForWrite() if necessary.
        background_work_finished_signal_.SignalAll();
      }
      mutex_.Unlock();
      imm_micros += (env_->NowMicros() - imm_start);
    }

    Slice key = input->key();
    if (compact->compaction->ShouldStopBefore(key) &&
        compact->builder != nullptr) {
      status = FinishCompactionOutputFile(compact, input);
      if (!status.ok()) {
        break;
      }
    }

    // Handle key/value, add to state, etc.
    bool drop = false;
    if (!ParseInternalKey(key, &ikey)) {
      // Do not hide error keys
      current_user_key.clear();
      has_current_user_key = false;
      last_sequence_for_key = kMaxSequenceNumber;
    } else {
      if (!has_current_user_key ||
          user_comparator()->Compare(ikey.user_key, Slice(current_user_key)) !=
              0) {
        // First occurrence of this user key
        current_user_key.assign(ikey.user_key.data(), ikey.user_key.size());
        has_current_user_key = true;
        last_sequence_for_key = kMaxSequenceNumber;
      }

      if (last_sequence_for_key <= compact->smallest_snapshot) {
        // Hidden by an newer entry for same user key
        drop = true;  // (A)
      } else if (ikey.type == kTypeDeletion &&
                 ikey.sequence <= compact->smallest_snapshot &&
                 compact->compaction->IsBaseLevelForKey(ikey.user_key)) {
        // For this user key:
        // (1) there is no data in higher levels
        // (2) data in lower levels will have larger sequence numbers
        // (3) data in layers that are being compacted here and have
        //     smaller sequence numbers will be dropped in the next
        //     few iterations of this loop (by rule (A) above).
        // Therefore this deletion marker is obsolete and can be dropped.
        drop = true;
      }

      last_sequence_for_key = ikey.sequence;
    }

    if (!drop) {
      // Open output file if necessary
      if (compact->builder == nullptr) {
        status = OpenCompactionOutputFile(compact);
        if (!status.ok()) {
          break;
        }
      }
      if (compact->builder->NumEntries() == 0) {
        compact->current_output()->smallest.DecodeFrom(key);
      }
      compact->current_output()->largest.DecodeFrom(key);
      compact->builder->Add(key, input->value());

      // Close output file if it is big enough
      if (compact->builder->FileSize() >=
          compact->compaction->MaxOutputFileSize()) {
        status = FinishCompactionOutputFile(compact, input);
        if (!status.ok()) {
          break;
        }
      }
    }

    input->Next();
  }
```

ä¸€ä¸ªå·¨å¤§çš„å¾ªç¯ã€‚é¦–å…ˆåˆ¤æ–­æ˜¯å¦å·²ç» `shutting_down_`ï¼Œå¦‚æœå·²ç»å…³é—­äº†ï¼Œåˆ™ç»ˆæ­¢å½“å‰çš„ Compaction è¿‡ç¨‹ï¼›éšååˆ¤æ–­å½“å‰æ˜¯å¦æœ‰ `imm_`ï¼Œå¦‚æœå­˜åœ¨çš„è¯åˆ™ä¹Ÿå…ˆæ‰§è¡Œ `CompactMemTable`ï¼›å†æ¥åˆ¤æ–­å½“å‰è¾“å‡ºçš„æ–‡ä»¶æ˜¯å¦å¯ä»¥ç»“æŸäº†ï¼Œå¦‚æœæ˜¯çš„è¯å°±æ‰§è¡Œ `FinishCompactionOutputFile` å®Œæˆå½“å‰æ–‡ä»¶ã€‚

æ¥ä¸‹æ¥æ˜¯æ˜¯å¦ä¸¢å¼ƒé”®å€¼å¯¹çš„åˆ¤å®šã€‚å¦‚æœæŸä¸ª `user_key` çš„éæœ€æ–°ç‰ˆæœ¬å°äºå¿«ç…§ç‰ˆæœ¬ï¼Œåˆ™å¯ä»¥ç›´æ¥ä¸¢å¼ƒï¼Œå› ä¸ºè¯»æœ€æ–°çš„ç‰ˆæœ¬å°±è¶³å¤Ÿäº†ï¼›å¦‚æœæŸä¸ªåˆ é™¤æ“ä½œçš„ç‰ˆæœ¬å°äºå¿«ç…§ç‰ˆæœ¬ï¼Œå¹¶ä¸”åœ¨æ›´é«˜å±‚æ²¡æœ‰ç›¸åŒçš„ `user_key`ï¼Œé‚£ä¹ˆè¿™ä¸ªåˆ é™¤æ“ä½œåŠå…¶ä¹‹å‰æ›´æ—©çš„æ’å…¥æ“ä½œå¯ä»¥åŒæ—¶ä¸¢å¼ƒäº†ã€‚

å¯¹äºæ²¡æœ‰ä¸¢å¼ƒçš„é”®å€¼å¯¹ï¼Œå°†å…¶å†™å…¥å½“å‰çš„ Table Builderã€‚å½“è¾“å‡ºçš„å¤§å°è¶…è¿‡é˜ˆå€¼ï¼ŒåŒæ ·æ‰§è¡Œ `FinishCompactionOutputFile`ï¼š

```c++
Status DBImpl::OpenCompactionOutputFile(CompactionState* compact) {
  assert(compact != nullptr);
  assert(compact->builder == nullptr);
  uint64_t file_number;
  {
    mutex_.Lock();
    file_number = versions_->NewFileNumber();
    pending_outputs_.insert(file_number);
    CompactionState::Output out;
    out.number = file_number;
    out.smallest.Clear();
    out.largest.Clear();
    compact->outputs.push_back(out);
    mutex_.Unlock();
  }

  // Make the output file
  std::string fname = TableFileName(dbname_, file_number);
  Status s = env_->NewWritableFile(fname, &compact->outfile);
  if (s.ok()) {
    compact->builder = new TableBuilder(options_, compact->outfile);
  }
  return s;
}

Status DBImpl::FinishCompactionOutputFile(CompactionState* compact,
                                          Iterator* input) {
  assert(compact != nullptr);
  assert(compact->outfile != nullptr);
  assert(compact->builder != nullptr);

  const uint64_t output_number = compact->current_output()->number;
  assert(output_number != 0);

  // Check for iterator errors
  Status s = input->status();
  const uint64_t current_entries = compact->builder->NumEntries();
  if (s.ok()) {
    s = compact->builder->Finish();
  } else {
    compact->builder->Abandon();
  }
  const uint64_t current_bytes = compact->builder->FileSize();
  compact->current_output()->file_size = current_bytes;
  compact->total_bytes += current_bytes;
  delete compact->builder;
  compact->builder = nullptr;

  // Finish and check for file errors
  if (s.ok()) {
    s = compact->outfile->Sync();
  }
  if (s.ok()) {
    s = compact->outfile->Close();
  }
  delete compact->outfile;
  compact->outfile = nullptr;

  if (s.ok() && current_entries > 0) {
    // Verify that the table is usable
    Iterator* iter =
        table_cache_->NewIterator(ReadOptions(), output_number, current_bytes);
    s = iter->status();
    delete iter;
    if (s.ok()) {
      Log(options_.info_log, "Generated table #%llu@%d: %lld keys, %lld bytes",
          (unsigned long long)output_number, compact->compaction->level(),
          (unsigned long long)current_entries,
          (unsigned long long)current_bytes);
    }
  }
  return s;
}
```

ç»§ç»­æ¥çœ‹ `DBImpl::DoCompactionWork`ï¼š

```c++
Status DBImpl::InstallCompactionResults(CompactionState* compact) {
  mutex_.AssertHeld();
  Log(options_.info_log, "Compacted %d@%d + %d@%d files => %lld bytes",
      compact->compaction->num_input_files(0), compact->compaction->level(),
      compact->compaction->num_input_files(1), compact->compaction->level() + 1,
      static_cast<long long>(compact->total_bytes));

  // Add compaction outputs
  compact->compaction->AddInputDeletions(compact->compaction->edit());
  const int level = compact->compaction->level();
  for (size_t i = 0; i < compact->outputs.size(); i++) {
    const CompactionState::Output& out = compact->outputs[i];
    compact->compaction->edit()->AddFile(level + 1, out.number, out.file_size,
                                         out.smallest, out.largest);
  }
  return versions_->LogAndApply(compact->compaction->edit(), &mutex_);
}

Status DBImpl::DoCompactionWork(CompactionState* compact) {
  ...
  if (status.ok() && shutting_down_.load(std::memory_order_acquire)) {
    status = Status::IOError("Deleting DB during compaction");
  }
  if (status.ok() && compact->builder != nullptr) {
    status = FinishCompactionOutputFile(compact, input);
  }
  if (status.ok()) {
    status = input->status();
  }
  delete input;
  input = nullptr;

  CompactionStats stats;
  stats.micros = env_->NowMicros() - start_micros - imm_micros;
  for (int which = 0; which < 2; which++) {
    for (int i = 0; i < compact->compaction->num_input_files(which); i++) {
      stats.bytes_read += compact->compaction->input(which, i)->file_size;
    }
  }
  for (size_t i = 0; i < compact->outputs.size(); i++) {
    stats.bytes_written += compact->outputs[i].file_size;
  }

  mutex_.Lock();
  stats_[compact->compaction->level() + 1].Add(stats);

  if (status.ok()) {
    status = InstallCompactionResults(compact);
  }
  if (!status.ok()) {
    RecordBackgroundError(status);
  }
  VersionSet::LevelSummaryStorage tmp;
  Log(options_.info_log, "compacted to: %s", versions_->LevelSummary(&tmp));
  return status;
}
```

æ‰§è¡Œ `InstallCompactionResults` æ—¶å°† Compaction çš„æ–‡ä»¶é›†åˆåŠ å…¥åˆ° `VersionEdit` çš„åˆ é™¤åˆ—è¡¨ä¸­ï¼Œå¹¶å°†æ–°ç”Ÿæˆçš„æ–‡ä»¶åŠ å…¥åˆ°æ–°æ–‡ä»¶åˆ—è¡¨é‡Œï¼Œéšåæ‰§è¡Œ `versions_->LogAndApply` æ›´æ–°ç‰ˆæœ¬ã€‚æœ€åå†æ‰§è¡Œä¸€äº›æ¸…ç†æ“ä½œï¼ŒCompaction è¿‡ç¨‹å°±ç»“æŸäº†ã€‚

### é¢˜å¤–è¯

çœ‹ `VersionSet::AddBoundaryInputs` éƒ¨åˆ†çš„ä»£ç æ—¶ï¼ŒVS Code ä¸Šæ˜¾ç¤ºæäº¤äº 4 å¹´å‰ï¼Œè€Œå¤§éƒ¨åˆ†çš„ LevelDB ä»£ç æäº¤äº 8 å¹´å‰ã€‚è¿™å¼•èµ·äº†æˆ‘çš„è­¦è§‰ï¼šè¿™ä¸ª Bug ç«Ÿç„¶å½±å“äº† 4 å¹´ã€‚éšå³ç”¨ VS Code çš„ [Git Blame æ’ä»¶](https://marketplace.visualstudio.com/items?itemName=waderyan.gitblame)æŸ¥çœ‹ä¿®å¤è¯¥ Bug å¯¹åº”çš„ [Commit](https://github.com/google/leveldb/commit/20fb601aa9f68ff0aa147df22524b7d01758552b) åŠå¯¹åº”çš„ [Pull Request](https://github.com/google/leveldb/pull/339)ï¼Œå‘ç°äº†ä¸å¾—äº†çš„äº‹æƒ…ï¼šè¿™ä¸ªæäº¤æ˜¯ 16 å¹´åˆçš„ï¼Œä½† 19 å¹´ 4 æœˆæ‰åˆå¹¶è¿›å»ã€‚

è¯¥ Bug æœ€æ—©æŠ¥å‘Šäº 2015 å¹´çš„ [Issue 320](https://github.com/google/leveldb/issues/320)ï¼Œå½“æ—¶ [richcole](https://github.com/richcole) å°±ç»™å‡ºäº† [Bug çš„åˆ†æ](https://github.com/google/leveldb/issues/320#issuecomment-160186125)ï¼Œå¹¶åœ¨ 16 å¹´åˆæäº¤äº†è¯¥ Bug çš„ä¿®å¤ï¼Œä½†ä¸€ç›´æ— äººç†ä¼šã€‚ç›´åˆ° 19 å¹´ 3 æœˆ [vonnyfly](https://github.com/vonnyfly) [å‘ç°äº†è¿™ä¸ªä¸¥é‡é—®é¢˜](https://github.com/google/leveldb/pull/339#issuecomment-471896373)ï¼Œè¿™æ‰å¼•èµ·äº†å®˜æ–¹çš„é‡è§†ï¼Œä¹‹ååœ¨å¤§å®¶çš„åä½œä¸‹ç»ˆäºå°†ä¿®å¤ patch åˆå¹¶åˆ°ä¸»åˆ†æ”¯ã€‚

è€ŒåŸºäº LevelDB å¼€å‘çš„ RocksDB åˆ™åœ¨è¯¥é—®é¢˜ä¸Šåšå‡ºäº†å¿«é€Ÿå“åº”ã€‚16 å¹´ [Issue 993](https://github.com/facebook/rocksdb/issues/993) ä¸­æœ‰äººè¯¢é—® RocksDB æ˜¯å¦åŒæ ·å—åˆ°è¯¥ Bug å½±å“ï¼ŒRocksDB çš„ä¸»è¦è´¡çŒ®è€… [igorcanadi](https://github.com/igorcanadi) åœ¨ 12 å°æ—¶å†…åŠæ—¶å›å¤ï¼Œ[è¡¨ç¤º RocksDB ä¸å—è¯¥ Bug å½±å“](https://github.com/facebook/rocksdb/issues/993#issuecomment-184830547)ï¼š

> I just read the issue more throughly. RocksDB doesn't have the same bug. Looks like we actually found and fixed that bug years ago. I don't know why we didn't contribute back to LevelDB :(

ç¬”è€…åªæ˜¯é€šè¿‡ GitHub çš„ Issue å’Œ PR æ¢å¤äº†è¯¥äº‹ä»¶çš„å‘å±•è¿‡ç¨‹ï¼Œä¸å¯¹æ­¤åšå‡ºä»»ä½•è¯„ä»·ã€‚ä¸è¿‡è¿™ä¸ªä¸¥é‡çš„ Bug åº”è¯¥ä»ç„¶å½±å“ç€å¾ˆå¤šé¡¹ç›®ï¼Œæ¯•ç«Ÿå°æ¦‚ç‡è§¦å‘æ¯” 100% è§¦å‘æ›´å¯æ€•ï¼Œå¸Œæœ›èƒ½å¼•èµ·å¤§å®¶çš„é‡è§†ï¼Œæ£€æŸ¥ä¸‹è‡ªå·±ä½¿ç”¨çš„ LevelDB æ˜¯ä¸æ˜¯ [Release 1.22](https://github.com/google/leveldb/releases/tag/1.22) ä¹‹å‰çš„ç‰ˆæœ¬ã€‚

> The Internet Never Forgets ğŸ˜„

### References

1. ["Compaction", *leveldb-handbook*](https://leveldb-handbook.readthedocs.io/zh/latest/compaction.html)
2. ["Fix snapshot compaction bug", *leveldb#339*](https://github.com/google/leveldb/pull/339)
3. ["Compaction causes data inconsistency when using snapshots", *leveldb#320*](https://github.com/google/leveldb/issues/320)
4. ["Dose rocksdb have the bug found in leveldb?", *rocksdb#993*](https://github.com/facebook/rocksdb/issues/993)