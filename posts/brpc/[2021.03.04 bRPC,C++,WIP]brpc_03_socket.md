# bRPC 源码分析「三、网络通信」

![bRPC 网络收发流程图 from 官方文档](../images/fc464d977f5fe4c4f5152fb14b02e9e4.png)

### 1. Single Connection

bRPC 支持短连接、连接池和单连接，前两种是非常通用的方案。而单连接是指进程内所有 client 与一台 server 最多只有一个连接，在该连接上同时处理多个请求，不要求回复返回顺序与请求发送顺序一致。

对于连接池，请求时 client 端从连接池中取出一个可用的连接并独占使用，写入请求，server 端在该连接上收到请求后进行处理，最后在该连接上写入回复。对该连接来说，读和写并不会同时发生，实际工作模式是半双工的。而对于单连接，可以连续地在该连接上写入请求并同时读取回复，工作模式是全双工的，不过需要有通过回复定位请求的能力，可以通过 UUID 解决。连续的写入和读取也可以形成更好的批量效果，减少系统调用次数。在多线程环境下，bRPC 的单连接读写操作做到了 [wait-free](http://en.wikipedia.org/wiki/Non-blocking_algorithm#Wait-freedom)。

### 2. Message Sending

“消息”是指向连接写出的有边界的二进制串。使用单连接的场景中，多线程可能会向同一个连接发送消息，该操作显然是非原子的，这时候就需要高效率地排队不同线程发送的数据包。bRPC 中使用了一个 MPSC 队列实现该需求，具体步骤如下：

1. 为每个连接维护一个 MPSC 的单向链表，当线程需要写消息时尝试获取该连接的独占写入权限（原子操作），成功获取权限的线程执行一次写入，而获取权限失败的线程仅将消息插入到该链表的头部（原子操作），并挂起等待回复；
2. 获得写入权限的线程根据连接对应的链表，批量地写入本线程以及其他线程发送的数据包，尽可能多的写入数据，但只写入一次防止本线程的请求产生过长的延迟，如果写入时连接的缓冲池已满无法写入，则启动一个新的 KeepWrite bthread 执行后续的写入操作；
3. KeepWrite bthread 负责将链表中的所有数据包写入连接直到链表为空，当连接缓冲池已满时则挂起等待 epoll 唤醒。

下面对照代码进行分析。单连接的实现位于 [src/brpc/socket.cpp](https://github.com/apache/incubator-brpc/blob/0.9.7/src/brpc/socket.cpp)，写入的流程为：

```c++
// socket.cpp，写入 IOBuf
int Socket::Write(butil::IOBuf *data, const WriteOptions *options_in) {
  WriteOptions opt;
  if (options_in) {
    opt = *options_in;
  }
  if (data->empty()) {
    return SetError(opt.id_wait, EINVAL);
  }
  if (opt.pipelined_count > MAX_PIPELINED_COUNT) {
    LOG(ERROR) << "pipelined_count=" << opt.pipelined_count << " is too large";
    return SetError(opt.id_wait, EOVERFLOW);
  }
  if (Failed()) {
    const int rc = ConductError(opt.id_wait);
    if (rc <= 0) {
      return rc;
    }
  }

  if (!opt.ignore_eovercrowded && _overcrowded) {
    return SetError(opt.id_wait, EOVERCROWDED);
  }

  // 对象池中获取一个 WriteRequest 对象
  WriteRequest *req = butil::get_object<WriteRequest>();
  if (!req) {
    return SetError(opt.id_wait, ENOMEM);
  }

  req->data.swap(*data);  // move 数据到 request 中
  // Set `req->next' to UNCONNECTED so that the KeepWrite thread will
  // wait until it points to a valid WriteRequest or NULL.
  // 先将 next 指针设为非法，后续判断依赖该操作
  req->next = WriteRequest::UNCONNECTED;
  req->id_wait = opt.id_wait;
  req->set_pipelined_count_and_user_message(opt.pipelined_count,
                                            DUMMY_USER_MESSAGE, opt.with_auth);
  return StartWrite(req, opt);  // 调用写入
}

int Socket::StartWrite(WriteRequest *req, const WriteOptions &opt) {
  // Release fence makes sure the thread getting request sees *req
  // 将链表头原子地替换为待写入的 request，原先的头部返回给 prev_head
  WriteRequest *const prev_head =
      _write_head.exchange(req, butil::memory_order_release);
  if (prev_head != NULL) {
    // Someone is writing to the fd. The KeepWrite thread may spin
    // until req->next to be non-UNCONNECTED. This process is not
    // lock-free, but the duration is so short(1~2 instructions,
    // depending on compiler) that the spin rarely occurs in practice
    // (I've not seen any spin in highly contended tests).
    // 如果 prev_head 非空，说明有其他线程拿到了权限在执行写入。
    // 此时将 next 指针设为 prev_head。
    // 该操作前 next 指向 WriteRequest::UNCONNECTED，写入的线程可以 spin 等待。
    req->next = prev_head;
    return 0;
  }

  int saved_errno = 0;
  bthread_t th;
  SocketUniquePtr ptr_for_keep_write;
  ssize_t nw = 0;

  // We've got the right to write.
  // 获得连接的写入权限，将指向改为 NULL
  // 下方所有操作均保证在单线程环境下执行
  req->next = NULL;

  // Connect to remote_side() if not.
  // 尝试连接对机
  int ret = ConnectIfNot(opt.abstime, req);
  if (ret < 0) {
    saved_errno = errno;
    SetFailed(errno, "Fail to connect %s directly: %m", description().c_str());
    goto FAIL_TO_WRITE;
  } else if (ret == 1) {
    // We are doing connection. Callback `KeepWriteIfConnected'
    // will be called with `req' at any moment after
    return 0;
  }

  // NOTE: Setup() MUST be called after Connect which may call app_connect,
  // which is assumed to run before any SocketMessage.AppendAndDestroySelf()
  // in some protocols(namely RTMP).
  req->Setup(this);  // 不确定功能，暂时搁置

  if (ssl_state() != SSL_OFF) {
    // Writing into SSL may block the current bthread, always write
    // in the background.
    // 对于 SSL，始终使用后台写入
    goto KEEPWRITE_IN_BACKGROUND;
  }

  // Write once in the calling thread. If the write is not complete,
  // continue it in KeepWrite thread.
  if (_conn) {
    butil::IOBuf *data_arr[1] = {&req->data};
    nw = _conn->CutMessageIntoFileDescriptor(fd(), data_arr, 1);
  } else {
    // 执行一次写入，默认 size_hint 为 1MB
    nw = req->data.cut_into_file_descriptor(fd());
  }
  if (nw < 0) {
    // RTMP may return EOVERCROWDED
    if (errno != EAGAIN && errno != EOVERCROWDED) {
      saved_errno = errno;
      // EPIPE is common in pooled connections + backup requests.
      PLOG_IF(WARNING, errno != EPIPE) << "Fail to write into " << *this;
      SetFailed(saved_errno, "Fail to write into %s: %s", description().c_str(),
                berror(saved_errno));
      goto FAIL_TO_WRITE;  // 失败时跳转
    }
  } else {
    AddOutputBytes(nw);
  }
  if (IsWriteComplete(req, true, NULL)) {
    // 判断所有写入完成，直接返回
    ReturnSuccessfulWriteRequest(req);
    return 0;
  }

KEEPWRITE_IN_BACKGROUND:
  // 写入未完成，启动后台 bthread 继续执行写入操作
  ReAddress(&ptr_for_keep_write);  // 转移当前 socket 对象所有权
  req->socket = ptr_for_keep_write.release();
  if (bthread_start_background(&th, &BTHREAD_ATTR_NORMAL, KeepWrite, req) !=
      0) {
    LOG(FATAL) << "Fail to start KeepWrite";
    // bthread 启动失败的情况下继续同步调用写入
    KeepWrite(req);
  }
  return 0;

FAIL_TO_WRITE:
  // `SetFailed' before `ReturnFailedWriteRequest' (which will calls
  // `on_reset' callback inside the id object) so that we immediately
  // know this socket has failed inside the `on_reset' callback
  ReleaseAllFailedWriteRequests(req);
  errno = saved_errno;
  return -1;
}

// iobuf_inl.h
inline ssize_t IOBuf::cut_into_file_descriptor(int fd, size_t size_hint) {
  return pcut_into_file_descriptor(fd, -1, size_hint);
}

// iobuf.cpp
ssize_t IOBuf::pcut_into_file_descriptor(int fd, off_t offset,
                                         size_t size_hint) {
  if (empty()) {
    return 0;
  }

  const size_t nref = std::min(_ref_num(), IOBUF_IOV_MAX);
  struct iovec vec[nref];  // 将 IOBuf 转为 iovec，批量写入
  size_t nvec = 0;
  size_t cur_len = 0;

  do {
    IOBuf::BlockRef const &r = _ref_at(nvec);
    vec[nvec].iov_base = r.block->data + r.offset;
    vec[nvec].iov_len = r.length;
    ++nvec;
    cur_len += r.length;
  } while (nvec < nref && cur_len < size_hint);  // size_hint 非精确限制

  ssize_t nw = 0;

  if (offset >= 0) {
    static iobuf::iov_function pwritev_func = iobuf::get_pwritev_func();
    nw = pwritev_func(fd, vec, nvec, offset);
  } else {
    nw = ::writev(fd, vec, nvec);  // 非阻塞批量写入
  }
  if (nw > 0) {
    pop_front(nw);  // 写入成功的部分 pop 掉
  }
  return nw;
}
```

`KeepWrite` 的流程为：

```c++
void *Socket::KeepWrite(void *void_arg) {
  g_vars->nkeepwrite << 1;
  WriteRequest *req = static_cast<WriteRequest *>(void_arg);
  SocketUniquePtr s(req->socket);  // 恢复 socket 的 unique_ptr

  // When error occurs, spin until there's no more requests instead of
  // returning directly otherwise _write_head is permantly non-NULL which
  // makes later Write() abnormal.
  WriteRequest *cur_tail = NULL;
  do {
    // req was written, skip it.
    if (req->next != NULL && req->data.empty()) {
      WriteRequest *const saved_req = req;
      req = req->next;
      s->ReturnSuccessfulWriteRequest(saved_req);
    }
    const ssize_t nw = s->DoWrite(req);  // 尝试执行写入
    if (nw < 0) {
      if (errno != EAGAIN && errno != EOVERCROWDED) {
        const int saved_errno = errno;
        PLOG(WARNING) << "Fail to keep-write into " << *s;
        s->SetFailed(saved_errno, "Fail to keep-write into %s: %s",
                     s->description().c_str(), berror(saved_errno));
        break;
      }
    } else {
      s->AddOutputBytes(nw);
    }
    // Release WriteRequest until non-empty data or last request.
    while (req->next != NULL && req->data.empty()) {
      WriteRequest *const saved_req = req;
      req = req->next;
      s->ReturnSuccessfulWriteRequest(saved_req);
    }
    // TODO(gejun): wait for epollout when we actually have written
    // all the data. This weird heuristic reduces 30us delay...
    // Update(12/22/2015): seem not working. better switch to correct code.
    // Update(1/8/2016, r31823): Still working.
    // Update(8/15/2017): Not working, performance downgraded.
    // if (nw <= 0 || req->data.empty()/*note*/) {
    if (nw <= 0) {
      g_vars->nwaitepollout << 1;
      bool pollin = (s->_on_edge_triggered_events != NULL);
      // NOTE: Waiting epollout within timeout is a must to force
      // KeepWrite to check and setup pending WriteRequests periodically,
      // which may turn on _overcrowded to stop pending requests from
      // growing infinitely.
      const timespec duetime =
          butil::milliseconds_from_now(WAIT_EPOLLOUT_TIMEOUT_MS);
      const int rc = s->WaitEpollOut(s->fd(), pollin, &duetime);
      if (rc < 0 && errno != ETIMEDOUT) {
        const int saved_errno = errno;
        PLOG(WARNING) << "Fail to wait epollout of " << *s;
        s->SetFailed(saved_errno, "Fail to wait epollout of %s: %s",
                     s->description().c_str(), berror(saved_errno));
        break;
      }
    }
    if (NULL == cur_tail) {
      for (cur_tail = req; cur_tail->next != NULL; cur_tail = cur_tail->next)
        ;
    }
    // Return when there's no more WriteRequests and req is completely
    // written.
    // 判断是否全部写完
    if (s->IsWriteComplete(cur_tail, (req == cur_tail), &cur_tail)) {
      CHECK_EQ(cur_tail, req);
      s->ReturnSuccessfulWriteRequest(req);
      return NULL;
    }
  } while (1);

  // Error occurred, release all requests until no new requests.
  s->ReleaseAllFailedWriteRequests(req);
  return NULL;
}

ssize_t Socket::DoWrite(WriteRequest *req) {
  // Group butil::IOBuf in the list into a batch array.
  butil::IOBuf *data_list[DATA_LIST_MAX];
  size_t ndata = 0;
  for (WriteRequest *p = req; p != NULL && ndata < DATA_LIST_MAX; p = p->next) {
    data_list[ndata++] = &p->data;  // 收集一批待写入的数据包后批量写入
  }

  if (ssl_state() == SSL_OFF) {
    // Write IOBuf in the batch array into the fd.
    if (_conn) {
      return _conn->CutMessageIntoFileDescriptor(fd(), data_list, ndata);
    } else {
      ssize_t nw = butil::IOBuf::cut_multiple_into_file_descriptor(
          fd(), data_list, ndata);
      return nw;
    }
  }

  CHECK_EQ(SSL_CONNECTED, ssl_state());
  if (_conn) {
    // TODO: Separate SSL stuff from SocketConnection
    return _conn->CutMessageIntoSSLChannel(_ssl_session, data_list, ndata);
  }
  int ssl_error = 0;
  ssize_t nw = butil::IOBuf::cut_multiple_into_SSL_channel(
      _ssl_session, data_list, ndata, &ssl_error);
  switch (ssl_error) {
  case SSL_ERROR_NONE:
    break;

  case SSL_ERROR_WANT_READ:
    // Disable renegotiation
    errno = EPROTO;
    return -1;

  case SSL_ERROR_WANT_WRITE:
    errno = EAGAIN;
    break;

  default: {
    const unsigned long e = ERR_get_error();
    if (e != 0) {
      LOG(WARNING) << "Fail to write into ssl_fd=" << fd() << ": "
                   << SSLError(ERR_get_error());
      errno = ESSL;
    } else {
      // System error with corresponding errno set
      PLOG(WARNING) << "Fail to write into ssl_fd=" << fd();
    }
    break;
  }
  }
  return nw;  // 返回成功写入的长度
}

// Check if there're new requests appended.
// If yes, point old_head to to reversed new requests and return false;
// If no:
//    old_head is fully written, set _write_head to NULL and return true;
//    old_head is not written yet, keep _write_head unchanged and return false;
// `old_head' is last new_head got from this function or (in another word)
// tail of current writing list.
// `singular_node' is true iff `old_head' is the only node in its list.
bool Socket::IsWriteComplete(Socket::WriteRequest *old_head, bool singular_node,
                             Socket::WriteRequest **new_tail) {
  CHECK(NULL == old_head->next);
  // Try to set _write_head to NULL to mark that the write is done.
  WriteRequest *new_head = old_head;
  WriteRequest *desired = NULL;
  bool return_when_no_more = true;
  if (!old_head->data.empty() || !singular_node) {
    // 当前写入链表还不能判断已经写完（当前节点非空或者链表不止一个节点）
    desired = old_head;
    // Write is obviously not complete if old_head is not fully written.
    return_when_no_more = false;
  }
  // CAS 检查是否存在需要写入的数据包
  if (_write_head.compare_exchange_strong(new_head, desired,
                                          butil::memory_order_acquire)) {
    // No one added new requests.
    if (new_tail) {
      *new_tail = old_head;
    }
    return return_when_no_more;
  }
  // CAS 失败，new_head 获得最新的写入链表头部
  CHECK_NE(new_head, old_head);
  // Above acquire fence pairs release fence of exchange in Write() to make
  // sure that we see all fields of requests set.

  // Someone added new requests.
  // Reverse the list until old_head.
  WriteRequest *tail = NULL;
  WriteRequest *p = new_head;
  // 翻转链表，从 new_head 到 old_head 翻转
  do {
    while (p->next == WriteRequest::UNCONNECTED) {
      // TODO(gejun): elaborate this
      // 如前文提到的，p->next 短时间内可能指向 UNCONNECTED，需要 spin 等待
      sched_yield();
    }
    WriteRequest *const saved_next = p->next;
    p->next = tail;
    tail = p;
    p = saved_next;
    CHECK(p != NULL);
  } while (p != old_head);

  // Link old list with new list.
  old_head->next = tail;
  // Call Setup() from oldest to newest, notice that the calling sequence
  // matters for protocols using pipelined_count, this is why we don't
  // calling Setup in above loop which is from newest to oldest.
  for (WriteRequest *q = tail; q; q = q->next) {
    q->Setup(this);
  }
  if (new_tail) {
    *new_tail = new_head;
  }
  return false;
}
```

### 3. Message Receiving

> WIP

### References

1. ["bRPC IO", *incubator-brpc*](https://github.com/apache/incubator-brpc/blob/master/docs/cn/io.md)

